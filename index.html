<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta property="og:title"
        content="Learning Multi-dimensional Human Preference for Text-to-Image Generation" />
    <meta property="og:url" content="https://wangbohan97.github.io/MPS/" />
    <meta property="og:image" content="static/images/banner1.jpeg" />
    <meta property="og:image:width" content="1200" />
    <meta property="og:image:height" content="630" />


    <meta name="viewport" content="width=device-width, initial-scale=1">


    <title>Learning Multi-dimensional Human Preference for Text-to-Image Generation</title>
    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

    <link rel="stylesheet" href="static/css/bulma.min.css">
    <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="static/css/index.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
    <script defer src="static/js/fontawesome.all.min.js"></script>
    <script src="static/js/bulma-carousel.min.js"></script>
    <script src="static/js/index.js"></script>
    <script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <script type="module" src="https://gradio.s3-us-west-2.amazonaws.com/4.1.1/gradio.js"></script>
</head>


<body>


    <section class="hero banner">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <div class="columns is-centered">
                    <div class="column has-text-centered">
                        <h1 class="title is-1 publication-title"><i class="ParaDiffusion-icon"></i><br />
                            Learning Multi-dimensional Human Preference for Text-to-Image Generation</h1>
                        <div class="is-size-5 publication-authors">
                            <!-- Paper authors -->
                            <span class="author-block">
                                Sixian Zhang<sup>*</sup>,</span>
                            <span class="author-block">
                                Bohan Wang<sup>*</sup>,</span>
                            <span class="author-block">
                                Junqiang Wu<sup>*</sup>,</span>
                            <span class="author-block">
                                Yan Li<sup>‡</sup>,</span>
                            <span class="author-block">
                                Tingting Gao,</span>
                            <span class="author-block">
                                Di Zhang,</span>
                            <span class="author-block">
                                Zhongyuan Wang</span>
                        </div>

                        <div class="is-size-5 publication-authors">
                            <span class="author-block">Kuaishou Technology</span>
                            
                            <span class="eql-cntrb"><small><br><sup>*</sup>Equal contribution. </small></span>
                            <span class="eql-cntrb"><small><br><sup>‡</sup>Corresponding author. </small></span>
                        </div>

                        <div class="column has-text-centered">
                            <div class="publication-links">
                                <span class="link-block">
                                    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Learning_Multi-Dimensional_Human_Preference_for_Text-to-Image_Generation_CVPR_2024_paper.pdf" target="_blank"   
                                        class="external-link button is-normal is-rounded is-white">
                                        <span class="icon">
                                            <i class="fas fa-file-pdf"></i>
                                        </span>
                                        <span>Paper</span>
                                    </a>
                                </span>

                                <span class="link-block">
                                    <a href="https://github.com/wangbohan97/MPS_model" target="_blank"
                                        class="external-link button is-normal is-rounded is-white">
                                        <span class="icon">
                                            <i class="fab fa-github"></i>
                                        </span>
                                        <span>Code</span>
                                    </a>
                                </span>
                                
                                <span class="link-block">
                                    <a href="https://arxiv.org/abs/2405.14705" target="_blank"
                                        class="external-link button is-normal is-rounded is-white">
                                        <span class="icon">
                                            <i class="ai ai-arxiv"></i>
                                        </span>
                                        <span>arXiv</span>
                                    </a>
                                </span>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
    </section>


    <!-- Paper abstract -->
    <section class="section hero is-light">
        <div class="container is-max-desktop">
            <div class="columns is-centered has-text-centered">
                <div class="column is-four-fifths">
                    <h2 class="title is-3">Abstract</h2>
                    <div class="content has-text-justified">
                        <p>
                            Current metrics for text-to-image models typically rely
                            on statistical metrics which inadequately represent the real
                            preference of humans. Although recent works attempt to
                            learn these preferences via human annotated images, they
                            reduce the rich tapestry of human preference to a single
                            overall score. However, the preference results vary when
                            humans evaluate images with different aspects. There-
                            fore, to learn the multi-dimensional human preferences,
                            we propose the Multi-dimensional Preference Score (MPS),
                            the first multi-dimensional preference scoring model for
                            the evaluation of text-to-image models. 
                            The MPS introduces the preference condition module upon CLIP model
                            to learn these diverse preferences. It is trained based on
                            our Multi-dimensional Human Preference (MHP) Dataset,
                            which comprises 918,315 human preference choices across
                            4 dimensions (i.e., aesthetics, semantic alignment, detail
                            quality and overall assessment) on 607,541 images. The im-
                            ages are generated by a wide range of latest text-to-image
                            models. The MPS outperforms existing scoring methods
                            across 3 datasets in 4 dimensions, enabling it a promising
                            metric for evaluating and improving text-to-image generation. 
                            The model and dataset will be made publicly available
                            to facilitate future research.
                        </p>
                    </div>
                </div>
            </div>
        </div>
    </section>
    <!-- End paper abstract -->
    
    
    

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <h2 class="title is-3">Multi-dimensional Human Preference (MHP) Dataset</h2>
                <h2>
                
                    To learn the multi-dimensional human preferences, we propose the Multi-dimensional Human Preference (MHP) dataset. 
                    Compared to prior efforts, the MHP dataset offers significant enhancements in prompts collection, image generation, and preference annotation.
                    <br>
                    (1) For the prompt collection, based on the categories schema of Parti, we annotate the collected prompts into 7 category labels (e.g., characters, scenes, objects, animals, etc.). 
                    For the underrepresented tail categories, we employ Large Language Models (LLMs) (e.g., GPT-4 \cite{GPT-4}) to generate additional prompts. 
                    This process results in a balanced prompt collection across various categories, which is used for later image generation.
                    <br>
                    (2) For image generation, we not only utilize existing open-source Diffusion models and their variants, but also employ GANs and auto-regressive models to generate images. 
                    Consequently, we generate a dataset of 607,541 images, which are further used to create 918,315 pairwise comparisons of images for preference annotation.
                    <br>
                    (3) For the annotation of human preferences, contrary to the single annotation of existing work, we consider a broader range of dimensions for human preferences and employ human annotators to label each image pair across four dimensions, including aesthetics, detail quality, semantic alignment, and overall score.
                    
                </h2>
            </div>
        </div>
    </section>

    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <table align="center" border="1" cellpadding="10" cellspacing="5" width="1000">
                    <caption><h2 class="title is-4">Comparisons of text-to-image models quality databases</h2></caption>
                    <tr>
                        <!-- <th width="80"></th> -->
                        <td rowspan="2">Dataset</td>
                        <td colspan="2">Prompt Collection</td>
                        <td colspan="2">Image Generation</td>
                        <td colspan="2">Preference Annotation</td>
                    </tr>
                    <tr>
                        <td>Source</td>
                        <td>Annotation</td>
                        <td>Source</td>
                        <td>Number</td>
                        <td>Rating</td>
                        <td>Dimension</td>
                    </tr>
                    <tr>
                        <td><a href="https://poloclub.github.io/diffusiondb/">DiffusionDB</a></td>
                        <td>DiffusionDB</td>
                        <td>×</td>
                        <td>Diffusion(1)</td>
                        <td>1,819,808</td>
                        <td>0</td>
                        <td>None</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/lcysyzxdxc/AGIQA-1k-Database">AGIQA-1K</a></td>
                        <td>DiffusionDB</td>
                        <td>×</td>
                        <td>Diffusion(2)</td>
                        <td>1,080</td>
                        <td>23,760</td>
                        <td>Overall</td>
                    </tr>
                    <tr>
                        <td><a href="https://stability.ai/research/pick-a-pic">PickScore</a></td>
                        <td>Web Application</td>
                        <td>×</td>
                        <td>Diffusion(3)</td>
                        <td>583,747</td>
                        <td>583,747</td>
                        <td>Overall</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/THUDM/ImageReward">ImageReward</a></td>
                        <td>DiffusionDB</td>
                        <td>×</td>
                        <td>Auto Regressive; Diffusion(6)</td>
                        <td>136,892</td>
                        <td>410,676</td>
                        <td>Overall</td>
                    </tr>
                    <tr>
                        <td><a href="https://tgxs002.github.io/align_sd_web/">HPS</a></td>
                        <td>DiffusionDB</td>
                        <td>×</td>
                        <td>Diffusion(1)</td>
                        <td>98,807</td>
                        <td>98,807</td>
                        <td>Overall</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/tgxs002/HPSv2">HPS v2</a></td>
                        <td>DiffusionDB, COCO</td>
                        <td>✓</td>
                        <td>GAN; Auto Regressive; Diffusion, COCO(9)</td>
                        <td>430,060</td>
                        <td>798,090</td>
                        <td>Overall</td>
                    </tr>
                    <tr>
                        <td><a href="https://github.com/lcysyzxdxc/AGIQA-3k-Database">AGIQA-3K</a></td>
                        <td>DiffusionDB</td>
                        <td>×</td>
                        <td>GAN; Auto Regressive; Diffusion(6)</td>
                        <td>2,982</td>
                        <td>125,244</td>
                        <td>Overall; Alignment</td>
                    </tr>
                    <tr>
                        <td>MHP(Ours)</td>
                        <td>DiffusionDB, PromptHero, KOLORS, GPT4</td>
                        <td>✓</td>
                        <td>GAN; Auto Regressive; Diffusion(9)</td>
                        <td>607,541</td>
                        <td>918,315</td>
                        <td>Aesthetics, Detail, Alignment, Overall</td>
                    </tr>
                </table>
            </div>
        </div>
    </section>


    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <h2 class="title is-3">Multi-dimensional Preference Score (MPS)</h2>
                <h2>

                    To learn human preferences, we propose the Multi-dimensional Preference Score (MPS), a unified model capable of predicting scores under various preference conditions.
                    <br>
                    (1) a certain preference is denoted by a series of descriptive words. For instance, the `aesthetic' condition is decomposed into words such as `light', `color', and `clarity' to describe the attributes of this condition.
                    <br>
                    (2) These attribute words are used to compute similarities with the prompt, resulting in a similarity matrix that reflects the correspondence between words in the prompt and the specified condition.
                    <br>
                    (3) Features from images and text are extracted using a pre-trained vision-language model. Subsequently, two modalities are fused through a multimodal cross-attention layer. 
                    <br>
                    (4) The similarity matrix serves as a mask merged into the cross-attention layer, which ensures that the text only related to the condition is attended to by the visual modality. Then the fused features are used to predict the preference scores.

                </h2>
                <br>
                <br>
                <img loading="lazy" src="static/images/framework_page-0001.jpg" alt="dog" />

            </div>
        </div>
    </section>
    

            </div>
        </div>
    </section> -->
    
    
    <section class="hero is-small">
        <div class="hero-body">
            <div class="container is-max-desktop">
                <h2 class="title is-3">Visualization Results</h2>
                <h2>
                    The visualization results indicate that our HPS attends to different regions of prompts
                    and images depending on the specific preference condition.
                    This is attributed to the condition mask, which allows only those words in 
                    the prompt related to the preference condition to be observed by the image. 
                    The condition mask ensures that the model predicts the preference with different inputs, 
                    and the model only needs to calculate the similarity between patches in the image 
                    and the retained partial prompt to determine the final score. 
                    Therefore, the selective focus enabled by the condition mask allows utilizing
                    a unified model to predict multinational preferences effectively, 
                    even if some preferences have weak correlations with others.
                </h2>
                <br>
                <div class="columns is-centered has-text-centered">
                    <div class="column">
                        <div id="modal-sample1" class="modal">
                            <div class="modal-background"></div>
                            <div class="modal-card">
                                <img loading="lazy" src="static/images/example_1.png" style="max-width: 100%"/>
                            </div>
                            <button class="modal-close is-large" aria-label="close"></button>
                        </div>
                        <div class="card">
                            <div class="card-image js-modal-trigger" data-target="modal-sample1">
                                <img loading="lazy" src="static/images/example_1.png" style="max-width: 100%" alt="sample1" />
                            </div>
                        </div>
                    </div>
                    <div class="column">
                        <div id="modal-sample2" class="modal">
                            <div class="modal-background"></div>
                            <div class="modal-content">
                                <img loading="lazy" src="static/images/example_2.png" style="max-width: 100%" />
                            </div>
                            <button class="modal-close is-large" aria-label="close"></button>
                        </div>
                        <div class="card">
                            <div class="card-image js-modal-trigger" data-target="modal-sample2">
                                <img loading="lazy" src="static/images/example_2.png" style="max-width: 100%" alt="sample2">
                            </div>
                        </div>
                    </div>
                </div>
                <div class="columns is-centered has-text-centered">
                    <div class="column">
                        <div id="modal-sample3" class="modal">
                            <div class="modal-background"></div>
                            <div class="modal-content">
                                <img loading="lazy" src="static/images/example_3.png" style="max-width: 100%" />
                            </div>
                            <button class="modal-close is-large" aria-label="close"></button>
                        </div>
                        <div class="card">
                            <div class="card-image js-modal-trigger" data-target="modal-sample3">
                                <img loading="lazy" src="static/images/example_3.png" style="max-width: 100%" alt="sample3">
                            </div>
                        </div>
                    </div>
                    <div class="column">
                        <div id="modal-sample4" class="modal">
                            <div class="modal-background"></div>
                            <div class="modal-content">
                                <img loading="lazy" src="static/images/example_4.png" style="max-width: 100%" />
                            </div>
                            <button class="modal-close is-large" aria-label="close"></button>
                        </div>
                        <div class="card">
                            <div class="card-image js-modal-trigger" data-target="modal-sample4">
                                <img loading="lazy" src="static/images/example_4.png" style="max-width: 100%" alt="sample4">
                            </div>
                        </div>
                    </div>
                </div>


   <section class="section" id="BibTeX">
       <div class="container is-max-desktop content">
           <h2 class="title">BibTeX</h2>
           <pre><code>@inproceedings{MPS,
            title={Learning Multi-dimensional Human Preference for Text-to-Image Generation},
            author={Zhang, Sixian and Wang, Bohan and Wu, Junqiang and Li, Yan and Gao, Tingting and Zhang, Di and Wang, Zhongyuan},
            booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
            pages={8018--8027},
            year={2024}
          }</code></pre>
       </div>
   </section>
   
            </div>
        </div>
    </section>


</body>

</html>
